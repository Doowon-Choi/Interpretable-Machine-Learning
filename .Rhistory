data("Boston", package = "MASS")
head(Boston)
install.packages("iml")
library("iml")
install.packages("iml")
install.packages("prediction")
library("tools")
removeDepends <- function(pkg, recursive = FALSE){
d <- package_dependencies(,installed.packages(), recursive = recursive)
depends <- if(!is.null(d[[pkg]])) d[[pkg]] else character()
needed <- unique(unlist(d[!names(d) %in% c(pkg,depends)]))
toRemove <- depends[!depends %in% needed]
if(length(toRemove)){
toRemove <- select.list(c(pkg,sort(toRemove)), multiple = TRUE,
title = "Select packages to remove")
remove.packages(toRemove)
return(toRemove)
} else {
invisible(character())
}
}
removeDepends("prediction")
library('iml')
install.packages("iml")
removeDepends("INLA")
library('iml')
install.packages("iml")
remove.packages("INLA", lib="~/R/win-library/3.4")
install.packages("iml")
updateR()
library(installr)
updateR()
install.packages("iml")
install.packages("party")
install.packages("partykit")
library(iml)       # ML interprtation
library("iml", lib.loc="~/R/win-library/4.0")
install.packages("backports")
library("iml", lib.loc="~/R/win-library/4.0")
detach("package:iml", unload=TRUE)
install.packages("backports")
install.packages("C:/Users/Doowon/Downloads/backports_1.1.8.tar.gz", repos = NULL, type = "source")
require(devtools)
install_version("backports", version = "1.1.0")
install.packages("devtools")
require(devtools)
install_version("backports", version = "1.1.0")
remove.packages("devtools", lib="~/R/win-library/4.0")
library("iml", lib.loc="~/R/win-library/4.0")
install.package("backports")
install.packages("backports")
library("iml", lib.loc="~/R/win-library/4.0")
install.packages("h2o")
library("h2o", lib.loc="~/R/win-library/4.0")
library("iml", lib.loc="~/R/win-library/4.0")
install.packages("iml")
library("iml", lib.loc="~/R/win-library/3.5")
data("Boston", package = "MASS")
head(Boston)
install.packages("party")
install.packages("partykit")
library('iml')
#### Load data
data("Boston", package = "MASS")
head(Boston)
library("party")
mob_mod <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + b + ptratio,
control = mob_control(minsplit = 40), data = Boston,
model = linearModel)
head(Boston)
mob_mod <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + ptratio,
control = mob_control(minsplit = 40), data = Boston,
model = linearModel)
X <- Boston[which(names(Boston) != "medv")]
predictor <- Predictor$new(rf, data = X, y = Boston$medv)
imp <- FeatureImp$new(predictor, loss = "mae")
class(mob)
class(mob_mod)
install.packages("pdp")
library("partykit", lib.loc="~/R/win-library/3.5")
mob_mod <- lmtree(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + ptratio,
control = mob_control(minsplit = 40), data = Boston,
model = linearModel)
mob_mod <- lmtree(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + ptratio, data = Boston,
minsize = 40)
X <- Boston[which(names(Boston) != "medv")]
predictor <- Predictor$new(mob_mod, data = X, y = Boston$medv)
imp <- FeatureImp$new(predictor, loss = "mae")
warnings()
plot(imp)
?FeatureEffect
?Partial
pdp <- Partial$new(predictor, feature="lstat")
pdp
"deprecated"
pdp$plot()
library('iml')
library("party")
library("ggplot2")
pdp <- Partial$new(predictor, feature="lstat")
pdp$plot()
samp1 <- FeatureEffect$new(predictor, feature="lstat")
attributes(samp1)
samp1$plot()
samp1$FeatureEffect
samp1$R6
samp1$InterpretationMethod
samp1
pdp_plot = pdp$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
pdp_plot
ice <- FeatureEffect$new(predictor, feature="lstat", method = "ice")
ice_plot = pdp$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
ice_plot
pdp2 <- FeatureEffect$new(predictor, feature="rm", method = "pdp")
pdp_plot2 = pdp2$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
gridExtra::grid.arrange(pdp_plot1, pdp_plot2, ncol = 2)
pdp_plot1 = pdp$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
gridExtra::grid.arrange(pdp_plot1, pdp_plot2, ncol = 2)
pdp <- FeatureEffect$new(predictor, feature="lstat", method = "pdp")
pdp_plot1 = pdp$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
pdp2 <- FeatureEffect$new(predictor, feature="rm", method = "pdp")
pdp_plot2 = pdp2$plot() +scale_x_continuous('rm') +
scale_y_continuous('Predicted medv')
gridExtra::grid.arrange(pdp_plot1, pdp_plot2, ncol = 2)
pdp_plot1
pdp <- FeatureEffect$new(predictor, feature="lstat", method = "pdp")
pdp_plot1 = pdp$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
pdp_plot1
pdp$plot()
ice1 <- FeatureEffect$new(predictor, feature="lstat", method = "ice")
ice_plot1 = ice1$plot() +scale_x_continuous('lstat') +
scale_y_continuous('Predicted medv')
ice2 <- FeatureEffect$new(predictor, feature="rm", method = "ice")
ice_plot2 = ice2$plot() +scale_x_continuous('rm') +
scale_y_continuous('Predicted medv')
gridExtra::grid.arrange(ice_plot1, ice_plot2, ncol = 2)
pdp_ice = FeatureEffect$new(predictor, feature="lstat", center.at = min(Boston$medv), method="pdp+ice")
pdp_ice$plot() + scale_color_discrete(guide='none')
pdp_ice$plot()
library("party")
library("ggplot2")
data("Boston", package = "MASS")
head(Boston)
mob_mod <- lmtree(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + ptratio, data = Boston,
minsize = 40)
### Using the iml Predictor() that holds the model and the data
X <- Boston[which(names(Boston) != "medv")]
predictor <- Predictor$new(mob_mod, data = X, y = Boston$medv)
library("partykit")
mob_mod <- lmtree(medv ~ lstat + rm | zn + indus + chas + nox + age +
dis + rad + tax + crim + ptratio, data = Boston,
minsize = 40)
### Using the iml Predictor() that holds the model and the data
X <- Boston[which(names(Boston) != "medv")]
predictor <- Predictor$new(mob_mod, data = X, y = Boston$medv)
pdp <- FeatureEffect$new(predictor, feature="lstat", method = "pdp")
pdp$plot()
ice1 <- FeatureEffect$new(predictor, feature="lstat", method = "ice")
ice1$plot()
pdp <- Partial$new(predictor, feature="lstat")
pdp$plot()
ice1$plot()
pdp_ice = FeatureEffect$new(predictor, feature="lstat", center.at = min(Boston$medv), method="pdp+ice")
pdp_ice$plot()
ale1 <- FeatureEffect$new(predictor, feature="lstat", method = "ale")
ale1$plot() + ggtile("ALE")
ale1$plot() + ggtitle("ALE")
pdp = FeatureEffect$new(pred, feature = "lstat", method = "pdp")
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = FeatureEffect$new(pred, feature = "rm", method = "pdp")
pdp2 = pdp$plot() + ggtitle("PDP")
ale1 = FeatureEffect$new(pred, feature = "lstat", method = "ale")$plot() + ggtitle("ALE")
ale2 = FeatureEffect$new(pred, feature = "rm", method = "ale")$plot() + ggtitle("ALE")
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
pdp = FeatureEffect$new(predictor, feature = "lstat", method = "pdp")
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = FeatureEffect$new(predictor, feature = "rm", method = "pdp")
pdp2 = pdp$plot() + ggtitle("PDP")
ale1 = FeatureEffect$new(predictor, feature = "lstat", method = "ale")$plot() + ggtitle("ALE")
ale2 = FeatureEffect$new(predictor, feature = "rm", method = "ale")$plot() + ggtitle("ALE")
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
install.packages("randomForest")
library("randomForest")
rf <- randomForest(medv ~ ., data = Boston, ntree = 50)
install.packages("e1071")
library("e1071")
head(Boston)
x_set <- subset(Boston, select=-medv)
y_set <- Boston$medv
x_set <- subset(Boston, select=-medv)
y_set <- Boston$medv
svm_tune <- tune(svm, train.x = x_set, train.y = y_set, kernel="radial",  ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
print(svm_tune)
10^(-1:2)
(-1:2:4)
(1:3)
(1:3, by = 0.5)
10^seq(-1:2)
seq(-1:2)
seq(-1:2, by = -0.5)
10^seq(-1,2)
c(.5,1,2)
svm_tune <- tune(svm, train.x = x_set, train.y = y_set, kernel="radial",  ranges=list(cost=10^seq(-1,2,0.5),
gamma=seq(.5,2,0.2)))
print(svm_tune)
svm_mod = svm(medv ~. data = Boston, kernel="radial", cost = 3.162, gamma = 0.5)
svm_mod = svm(medv ~., data = Boston, kernel="radial", cost = 3.162, gamma = 0.5)
X
predictor.rf <- Predictor$new(rf, data = X, y=Boston$medv)
predictor.svm <- Predictor$new(svm_mod, data = X, y = Boston$medv)
imp.rf <- FeatureImp$new(predictor.rf, loss="mse")
imp.svm <- FeatureImp$new(predictorsvm, loss="mse")
p1 <- plot(imp.rf) + ggtile("random forest")
p2 <- plot(imp.svm) + ggtitle("SVM")
gridExtra::grid.arrange(p1, p2, nrow = 1)
p1 <- plot(imp.rf) + ggtitle("random forest")
p2 <- plot(imp.svm) + ggtitle("SVM")
gridExtra::grid.arrange(p1, p2, nrow = 1)
p1 <- plot(imp.rf) + ggtitle("random forest")
p2 <- plot(imp.svm) + ggtitle("SVM")
imp.svm <- FeatureImp$new(predictorsvm, loss="mse")
imp.rf <- FeatureImp$new(predictor.rf, loss="mse")
imp.svm <- FeatureImp$new(predictor.svm, loss="mse")
p1 <- plot(imp.rf) + ggtitle("random forest")
p2 <- plot(imp.svm) + ggtitle("SVM")
gridExtra::grid.arrange(p1, p2, nrow = 1)
rf.pdp <- FeatureEffect$new(predictor.rf, feature = "rm", method = "pdp")
svm.pdp <- FeatureEffect$new(predictor.svm, feature = "rm", method = "pdp")
p1.pdp <- plot(rf.pdp) + ggtitle("random forest")
p2.pdp <- plot(svm.pdp) + ggtitle("SVM")
gridExtra::grid.arrange(p1.pdp, p2.pdp, nrow = 1)
rf.ice <- FeatureEffect$new(predictor.rf, feature = "rm", method = "ice")
svm.ice <- FeatureEffect$new(predictor.svm, feature = "rm", method = "ice")
rf.ice <- FeatureEffect$new(predictor.rf, feature = "rm", method = "ice")
svm.ice <- FeatureEffect$new(predictor.svm, feature = "rm", method = "ice")
p1.ice <- plot(rf.ice) + ggtitle("random forest")
p2.ice <- plot(svm.ice) + ggtitle("SVM")
gridExtra::grid.arrange(p1.ice, p2.ice, nrow = 1)
p1.ice <- plot(rf.ice) + ggtitle("pdp-rf")
p2.ice <- plot(svm.ice) + ggtitle("pdp-SVM")
ale.rf = FeatureEffect$new(predictor.rf, feature = "rm", method = "ale")$plot() + ggtitle("ale-rf")
ale.svm = FeatureEffect$new(predictor.svm, feature = "rm", method = "ale")$plot() + ggtitle("ale-svm")
gridExtra::grid.arrange(p1.pdd, p2pdp, ale.rf, ale.svm)
gridExtra::grid.arrange(p1.pdp, p2pdp, ale.rf, ale.svm)
gridExtra::grid.arrange(p1.pdp, p2,pdp, ale.rf, ale.svm)
rf.ale = FeatureEffect$new(predictor.rf, feature = "rm", method = "ale")
svm.ale = FeatureEffect$new(predictor.svm, feature = "rm", method = "ale")
p1.ale <- plot(rf.ale) + ggtitle("ale-rf")
p2.ale <- plot(svm.ale) + ggtitle("ale-svm")
gridExtra::grid.arrange(p1.pdp, p2,pdp, p1.ale, p2.ale)
p1.ale
p2.ale
gridExtra::grid.arrange(p1.pdp, p2,pdp, p1.ale, p2.ale, nrow=2)
gridExtra::grid.arrange(p1.pdp, p2,pdp, p1.ale, p2.ale, nrow=1)
p <- list()
p[[1]] <- p1.ice
p[[2]] <- p2.ice
p[[3]] <- p1.ale
p[[4]] <- p2.ale
do.call("grid.arrange", c(p,ncol=2))
rf.pdp <- FeatureEffect$new(predictor.rf, feature = "rm", method = "pdp")
svm.pdp <- FeatureEffect$new(predictor.svm, feature = "rm", method = "pdp")
p1.pdp <- plot(rf.pdp) + ggtitle("random forest")
p2.pdp <- plot(svm.pdp) + ggtitle("SVM")
p <- list()
p[[1]] <- p1.ice
p[[2]] <- p2.ice
p[[3]] <- p1.ale
p[[4]] <- p2.ale
do.call("grid.arrange", c(p,ncol=2))
p1.pdp
p[[1]] <- p1.pdp
p[[2]] <- p2.pdp
p[[3]] <- p1.ale
p[[4]] <- p2.ale
do.call("grid.arrange", c(p,ncol=2))
pdp_obj2 <- FeatureEffect$new(predictor.rf, feature = C("lstat","rm"), method = "pdp")
pdp_obj2 <- FeatureEffect$new(predictor.rf, feature = c("lstat","rm"), method = "pdp")
pdp_obj2$plot()
inter_all <- Interaction$new(svm_mod)
inter_all <- Interaction$new(predictor.svm)
inter_all$plot() + ggtitle("svm")
inter_all$plot() + ggtitle("svm")
inter_rad <- Interaction$new(predictor.svm, feature="rad")
inter_rad$plot() + ggtitle("interaction with rad")
?TreeSurrogate
interaction_pdp <- Partial$new(
components_iml,
c("lstat", "rm"),
ice = FALSE,
grid.size = 20
)
plot(interaction_pdp)
interaction_pdp <- Partial$new(predictor.svm, c("lstat", "rm"), ice = FALSE, grid.size = 20)
plot(interaction_pdp)
tree <- TreeSurrogate$new(predictor.svm, maxdepth = 3)
tree$r.squared
plot(tree)
library("partykit", lib.loc="~/R/win-library/3.5")
tree <- TreeSurrogate$new(predictor.svm, maxdepth = 3, method = "ctree")
tree$results
local_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10)
install.packages("glmnet")
local_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10)
library("glmnet", lib.loc="~/R/win-library/3.5")
detach("package:glmnet", unload=TRUE)
install.packages("foreach")
library("glmnet", lib.loc="~/R/win-library/3.5")
local_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10)
install.packages("C:/Users/Doowon/Downloads/glmnet_3.0.tar.gz", repos = NULL, type = "source")
install.packages("shape")
install.packages("C:/Users/Doowon/Downloads/glmnet_3.0.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/Doowon/Downloads/glmnet_3.0-1.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/Doowon/Downloads/glmnet_2.0-18.tar.gz", repos = NULL, type = "source")
library("glmnet", lib.loc="~/R/win-library/3.5")
local_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10)
install.packages("gower")
library("glmnet", lib.loc="~/R/win-library/3.5")
local_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10)
local_mod$plot()
local_mod$predict(x_set[2,])
local_mod
head(Boston)
LocalModel
?LocalModel
local_mod$explain( x_set[2,])
install.packages("lime")
library(lime)
data(biopsy)
biopsy$ID <- NULL
biopsy <- na.omit(biopsy)
names(biopsy) <- c('clump thickness', 'uniformity of cell size',
'uniformity of cell shape', 'marginal adhesion',
'single epithelial cell size', 'bare nuclei',
'bland chromatin', 'normal nucleoli', 'mitoses',
'class')
set.seed(4)
test_set <- sample(seq_len(nrow(biopsy)), 100)
class_value <- biopsy$class
biopsy$class <- NULL
model <- lda(biopsy[-test_set, ], class_value[-test_set])
train_x <- biopsy[-test_set, ]
train_y <- class_value[-test_set]
model <- lda(train_x, train_y)
model_lda <- lda(train_x, train_y)
lda_mod <- lda(train_x, train_y)
?LIME
?lime
explainer <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE)
?explain
head(biopsy)
data(biopsy)
# First we'll clean up the data a bit
biopsy$ID <- NULL
biopsy <- na.omit(biopsy)
names(biopsy) <- c('clump thickness', 'uniformity of cell size',
'uniformity of cell shape', 'marginal adhesion',
'single epithelial cell size', 'bare nuclei',
'bland chromatin', 'normal nucleoli', 'mitoses',
'class')
head(biopsy)
biopsy[test_set[1:4], ]
explanation <- explain(biopsy[test_set[1:4], ], explainer,labels='benign', n_features = 4
, feature_select = "highest_weights")
# Now we'll fit a linear discriminant model on all but 4 cases
set.seed(4)
test_set <- sample(seq_len(nrow(biopsy)), 100)
class_value <- biopsy$class
biopsy$class <- NULL
train_x <- biopsy[-test_set, ]
train_y <- class_value[-test_set]
lda_mod <- lda(train_x, train_y)
#### Start line
### train the explainer // similar as predictor in package 'iml'
explainer <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE)
# Use the explainer on new observations
#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them
# must be specified.
## ridge is used (i.e., highest weights)
explanation <- explain(biopsy[test_set[1:4], ], explainer,labels='benign', n_features = 4
, feature_select = "highest_weights")
tibble::glimpse(explanation_caret)
### train the explainer // similar as predictor in package 'iml'
explainer_lda <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE)
explanation_lda <- explain(biopsy[test_set[1:4], ], explainer_lda,labels='benign', n_features = 4
, feature_select = "highest_weights")
tibble::glimpse(explanation)
plot_features(explanation_lda)
explanation_lda <- explain(biopsy[test_set[1:4], ], explainer_lda,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
explanation[, 2:9]
plot_features(explanation_lda)
plot_features(explanation_lda)
plot_features(explanation_lda, ncol = 1)
biopsy[test_set[1:4], ]
?lime
explainer_lda <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =10)
# Use the explainer on new observations
#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them
# must be specified.
## ridge is used (i.e., highest weights)
explanation_lda <- explain(biopsy[test_set[1:4], ], explainer_lda,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
explanation[, 2:9]
#tibble::glimpse(explanation)
plot_features(explanation_lda, ncol = 1)
#### Start line
### train the explainer // similar as predictor in package 'iml'
explainer_lda_5 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =10)
explainer_lda_10 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =10)
# Use the explainer on new observations
#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them
# must be specified.
## ridge is used (i.e., highest weights)
explanation_lda_5 <- explain(biopsy[test_set[1:4], ], explainer_lda_5,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
explanation_lda_10 <- explain(biopsy[test_set[1:4], ], explainer_lda_10,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
par(mfrow=c(1,2))
plot_features(explanation_lda_5, ncol = 1)
plot_features(explanation_lda_10, ncol = 1)
par(mfrow=c(1,2))
plot_features(explanation_lda_5, ncol = 1)
plot_features(explanation_lda_10, ncol = 1)
plot_features(explanation_lda_5, ncol = 1)
explainer_lda_5 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =5)
explainer_lda_10 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =10)
# Use the explainer on new observations
#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them
# must be specified.
## ridge is used (i.e., highest weights)
explanation_lda_5 <- explain(biopsy[test_set[1:4], ], explainer_lda_5,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
explanation_lda_10 <- explain(biopsy[test_set[1:4], ], explainer_lda_10,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
par(mfrow=c(1,2))
plot_features(explanation_lda_5, ncol = 1)
plot_features(explanation_lda_10, ncol = 1)
par(mfrow=c(2,1))
plot_features(explanation_lda_5, ncol = 1)
plot_features(explanation_lda_10, ncol = 1)
par(mfrow=c(2,1))
plot_features(explanation_lda_5)
plot_features(explanation_lda_10)
plot_features(explanation_lda_5)
plot_features(explanation_lda_10)
explainer_lda_4 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =4)
explainer_lda_8 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =8)
# Use the explainer on new observations
#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them
# must be specified.
## ridge is used (i.e., highest weights)
explanation_lda_4 <- explain(biopsy[test_set[1:4], ], explainer_lda_4,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
explanation_lda_8 <- explain(biopsy[test_set[1:4], ], explainer_lda_8,labels='benign', n_features = 4
, feature_select = "highest_weights", kernel_width = 0.5)
plot_features(explanation_lda_4)
plot_features(explanation_lda_8)
class(plot_feautres(explanation_lda_10))
class(plot_features(explanation_lda_10))
plot_bin4 <- plot_features(explanation_lda_4)
plot_bin8 <- plot_features(explanation_lda_8)
gridExtra::grid.arrange(plot_bin4, plot_bin8, nrow=1)
gridExtra::grid.arrange(plot_bin4, plot_bin8, nrow=2)
plot_bin4 <- plot_features(explanation_lda_4, ncol = 1)
plot_bin8 <- plot_features(explanation_lda_8, ncol = 1)
gridExtra::grid.arrange(plot_bin4, plot_bin8, nrow=2)
gridExtra::grid.arrange(plot_bin4, plot_bin8, ncol=2)
plot_explanations(explanation_lda_4)
