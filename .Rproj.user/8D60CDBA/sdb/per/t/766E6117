{
    "collab_server" : "",
    "contents" : "####### Useful online link for stuyding Explainable AI #############\n\n## https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606\n## https://www.shirin-glander.de/2018/07/explaining_ml_models_code_caret_iml/\n## https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/05.8-agnostic-lime.Rmd\n## https://bradleyboehmke.github.io/HOML/iml.html#feature-interactions\n\nlibrary('iml')\nlibrary(\"partykit\")\nlibrary(\"ggplot2\")\n\n#### Load data\ndata(\"Boston\", package = \"MASS\")\nhead(Boston)\n\n## fit machine learning model using partykit ##\n\n#mob_mod <- mob(medv ~ lstat + rm | zn + indus + chas + nox + age + \n#              dis + rad + tax + crim + ptratio,\n#            control = mob_control(minsplit = 40), data = Boston, \n#            model = linearModel)\n\nmob_mod <- lmtree(medv ~ lstat + rm | zn + indus + chas + nox + age + \n                 dis + rad + tax + crim + ptratio, data = Boston, \n               minsize = 40)\n\n### Using the iml Predictor() that holds the model and the data\nX <- Boston[which(names(Boston) != \"medv\")]\npredictor <- Predictor$new(mob_mod, data = X, y = Boston$medv)\n\n### Feature Importance (a.k.a permutation approach)\nimp <- FeatureImp$new(predictor, loss = \"mae\")\nimp$results\nplot(imp)\n\n########### Feature effects, PDP, ALE, ICE #########\n### Partial dependence plot ##\n### in the folloiwng results, the yellow line shows PDP\npdp <- Partial$new(predictor, feature=\"lstat\")\npdp$plot()\n\n### PDP ###\npdp <- FeatureEffect$new(predictor, feature=\"lstat\", method = \"pdp\")\npdp_plot1 = pdp$plot() +scale_x_continuous('lstat') + \n  scale_y_continuous('Predicted medv')\n\npdp2 <- FeatureEffect$new(predictor, feature=\"rm\", method = \"pdp\")\npdp_plot2 = pdp2$plot() +scale_x_continuous('rm') + \n  scale_y_continuous('Predicted medv')\n\n\ngridExtra::grid.arrange(pdp_plot1, pdp_plot2, ncol = 2)\n\n\nPartial$new(predictor, c(\"lstat\", \"rm\"))\n\n### ICE ####\nice1 <- FeatureEffect$new(predictor, feature=\"lstat\", method = \"ice\")\nice_plot1 = ice1$plot() +scale_x_continuous('lstat') + \n  scale_y_continuous('Predicted medv')\n\nice2 <- FeatureEffect$new(predictor, feature=\"rm\", method = \"ice\")\nice_plot2 = ice2$plot() +scale_x_continuous('rm') + \n  scale_y_continuous('Predicted medv')\n\ngridExtra::grid.arrange(ice_plot1, ice_plot2, ncol = 2)\n\n#### pdp + ICE ### not recommended, use Partial$new to draw both pdp and ice\npdp_ice = FeatureEffect$new(predictor, feature=\"lstat\", center.at = min(Boston$medv), method=\"pdp+ice\")\npdp_ice$plot() + scale_color_discrete(guide='none')\n\n### Accumulated local effect, ALE ###\nale1 <- FeatureEffect$new(predictor, feature=\"lstat\", method = \"ale\")\nale1$plot() + ggtitle(\"ALE\")\n\n### compare PDP and ALE ####\npdp = FeatureEffect$new(predictor, feature = \"lstat\", method = \"pdp\")\npdp1 = pdp$plot() + ggtitle(\"PDP\")\npdp = FeatureEffect$new(predictor, feature = \"rm\", method = \"pdp\")\npdp2 = pdp$plot() + ggtitle(\"PDP\")\nale1 = FeatureEffect$new(predictor, feature = \"lstat\", method = \"ale\")$plot() + ggtitle(\"ALE\")\nale2 = FeatureEffect$new(predictor, feature = \"rm\", method = \"ale\")$plot() + ggtitle(\"ALE\")\n\ngridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)\n\n## if pdp is similar to ale, that measn two features, lstat and rm, are not correlated ##\n## both pdp and ale shows effect over the possible range of features, but some of feature values are unrealistic\n\n#### the following one is interaction plot, before that let's compare the result between SVM and random forest\n### train random forest\nlibrary(\"randomForest\")\nrf <- randomForest(medv ~ ., data = Boston, ntree = 50)\n\n### train svm using radial kernel and determine cost and gamma by cross validation\nlibrary(\"e1071\")\nx_set <- subset(Boston, select=-medv)\ny_set <- Boston$medv\n\n### tuning hyper-parameters of svm by 10-fold cross validation\nsvm_tune <- tune(svm, train.x = x_set, train.y = y_set, kernel=\"radial\",  ranges=list(cost=10^seq(-1,2,0.5), \n                                                                                      gamma=seq(.5,2,0.2)))\nprint(svm_tune)\nsvm_mod = svm(medv ~., data = Boston, kernel=\"radial\", cost = 3.162, gamma = 0.5)\n\n### create predictor from random forest and SVM ###\npredictor <- Predictor$new(mob_mod, data = X, y = Boston$medv)\n\npredictor.rf <- Predictor$new(rf, data = X, y=Boston$medv)\npredictor.svm <- Predictor$new(svm_mod, data = X, y = Boston$medv)\n\n### see the feature importance \nimp.rf <- FeatureImp$new(predictor.rf, loss=\"mse\")\nimp.svm <- FeatureImp$new(predictor.svm, loss=\"mse\")\n\np1 <- plot(imp.rf) + ggtitle(\"random forest\")\np2 <- plot(imp.svm) + ggtitle(\"SVM\")\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\npdp_obj2 <- FeatureEffect$new(predictor.rf, feature = c(\"lstat\",\"rm\"), method = \"pdp\")\n#### response values on the 2D of feature space\npdp_obj2$plot()\n\n### compare pdp\nrf.pdp <- FeatureEffect$new(predictor.rf, feature = \"rm\", method = \"pdp\")\nsvm.pdp <- FeatureEffect$new(predictor.svm, feature = \"rm\", method = \"pdp\")\n\np1.pdp <- plot(rf.pdp) + ggtitle(\"random forest\")\np2.pdp <- plot(svm.pdp) + ggtitle(\"SVM\")\n\ngridExtra::grid.arrange(p1.pdp, p2.pdp, nrow = 1)\n\n##### Compare ice\nrf.ice <- FeatureEffect$new(predictor.rf, feature = \"rm\", method = \"ice\")\nsvm.ice <- FeatureEffect$new(predictor.svm, feature = \"rm\", method = \"ice\")\n\np1.ice <- plot(rf.ice) + ggtitle(\"pdp-rf\")\np2.ice <- plot(svm.ice) + ggtitle(\"pdp-SVM\")\n\ngridExtra::grid.arrange(p1.ice, p2.ice, nrow = 1)\n\n### compare pdp and ale over two models with respect to variable rm\n\nrf.ale = FeatureEffect$new(predictor.rf, feature = \"rm\", method = \"ale\")\nsvm.ale = FeatureEffect$new(predictor.svm, feature = \"rm\", method = \"ale\")\n\np1.ale <- plot(rf.ale) + ggtitle(\"ale-rf\")\np2.ale <- plot(svm.ale) + ggtitle(\"ale-svm\")\n\n\ngridExtra::grid.arrange(p1.pdp, p2,pdp, p1.ale, p2.ale, nrow=1)\n\np <- list()\np[[1]] <- p1.pdp\np[[2]] <- p2.pdp\np[[3]] <- p1.ale\np[[4]] <- p2.ale\n\ndo.call(\"grid.arrange\", c(p,ncol=2)) \n\n############ Measuring interactions by SVM model ###############\n\n### interaction by H-statistic. the particular feature interacts with any other features. From this, we find the most\n### influential variable interacted with others.\ninter_all <- Interaction$new(predictor.svm)\ninter_all$plot() + ggtitle(\"svm\")\n\n## rad is selected variable with high degree of interaction. The next step is to calculate two-way interaction w.r.t 'rad'\ninter_rad <- Interaction$new(predictor.svm, feature=\"rad\")\ninter_rad$plot() + ggtitle(\"interaction with rad\")\n\n\n########### Surrogate model (interpretable model instead of black box model) using tree ##########\n#### In this case, we use ctree in partykit invented by Achime Zeilies,who also invented nice theoretical decision tree MOB\ntree <- TreeSurrogate$new(predictor.svm, maxdepth = 3)\ntree$r.squared\ntree$results\nplot(tree)\n\n######## Local interpretable model-agnostic, LIME #########\n#### See the behavior of the particular single instance and personally, This is similar as to locally weighted regression framework####\n\n#### LIME python version\n\n### Note that we have focused on global interpretability  #####\n#The generalized algorithm LIME applies is: http://uc-r.github.io/lime\n\n  #### Every complex model is linear on a local scale, like in traditional analysis, Taylor's expansion has\n  #### similar philosophy, rougly.\n\n## Lime procedure ##\n\n# Given an observation, permute it to create replicated feature data with slight value modifications.\n# Compute similarity distance measure between original observation and permuted observations.\n# Apply selected machine learning model to predict outcomes of permuted data.\n# Select m number of features to best describe predicted outcomes.\n# Fit a simple model to the permuted data, explaining the complex model outcome with m features from the permuted data weighted by its similarity to the original observation .\n# Use the resulting feature weights to explain local behavior.\n\n### There is another package \"lime\" for the same task, but \"iml\" pacakge is used below\n\nlibrary(lime)\nlocal_mod <- LocalModel$new(predictor.svm, x.interest = x_set[2,], k = 10) ### I guess LASSO is used here.\nlocal_mod$explain(x_set[2,])\n#local_mod$predict(x_set[2,])\nlocal_mod$plot()\n\n\n######## conduct LIME using \"lime\" package ###########\n### I follow the exisiting online material https://www.data-imaginist.com/2017/announcing-lime/ ####\n### http://uc-r.github.io/lime / good introduction to using lime package \n### However, I see the effect of bins that discretize the continuous features on the interpretation. This implies the\n### possible limitation of the method\n\n\ndata(biopsy)\n\n# First we'll clean up the data a bit\nbiopsy$ID <- NULL\nbiopsy <- na.omit(biopsy)\nnames(biopsy) <- c('clump thickness', 'uniformity of cell size', \n                   'uniformity of cell shape', 'marginal adhesion',\n                   'single epithelial cell size', 'bare nuclei', \n                   'bland chromatin', 'normal nucleoli', 'mitoses',\n                   'class')\n\n# Now we'll fit a linear discriminant model on all but 4 cases\nset.seed(4)\ntest_set <- sample(seq_len(nrow(biopsy)), 100)\nclass_value <- biopsy$class\nbiopsy$class <- NULL\n\ntrain_x <- biopsy[-test_set, ]\ntrain_y <- class_value[-test_set]\n\nlda_mod <- lda(train_x, train_y)\n\n#### Start line\n### train the explainer // similar as predictor in package 'iml'\nexplainer_lda_4 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =4)\nexplainer_lda_8 <- lime(train_x, model = lda_mod, bin_continuous = TRUE, quantile_bins = FALSE,  n_bins =8)\n\n# Use the explainer on new observations\n#n_lables = 2 explain the prob of 1 and 0, labels: which lable do you want to explain?. Either of them\n# must be specified.\n## ridge is used (i.e., highest weights)\nexplanation_lda_4 <- explain(biopsy[test_set[1:4], ], explainer_lda_4,labels='benign', n_features = 4\n                       , feature_select = \"highest_weights\", kernel_width = 0.5)\nexplanation_lda_8 <- explain(biopsy[test_set[1:4], ], explainer_lda_8,labels='benign', n_features = 4\n                           , feature_select = \"highest_weights\", kernel_width = 0.5)\n\n### draw plot\nplot_bin4 <- plot_features(explanation_lda_4, ncol = 1)\nplot_bin8 <- plot_features(explanation_lda_8, ncol = 1)\n\ngridExtra::grid.arrange(plot_bin4, plot_bin8, ncol=2)\nplot_explanations(explanation_lda_4)\n\n\n##### lime supports supervised models produced in caret, mlr, xgboost, h2o, keras, and MASS::lda.\n\n##### Use lime with unspported pacage in R http://uc-r.github.io/lime\n\n",
    "created" : 1596033920932.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3043236337",
    "id" : "766E6117",
    "lastKnownWriteTime" : 1596162353,
    "last_content_update" : 1596162353604,
    "path" : "~/Interpretable_AI/Interpretably_AI_code.R",
    "project_path" : "Interpretably_AI_code.R",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}